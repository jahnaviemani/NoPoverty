{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFg4yLOJxNXibVwLyX2FlU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhWD9IJ2XJ11",
        "outputId": "c8afc6dd-f0c8-4072-a75f-f8f5542afd28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.116.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.35.0)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.11/dist-packages (2.0.41)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.47.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.14.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy) (3.2.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.48.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.14.tar.gz (51.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.14.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.14-cp311-cp311-linux_x86_64.whl size=4237776 sha256=a12276e4f20e3743d1e5af3e65c491fec9f52a0cd70a08cf4e9031390a5c13a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/b6/cf/7315ec7b0149210d2d4447d9c3338b36d10e56a1ecddcd35c0\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.14\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas openpyxl fastapi nest-asyncio uvicorn sqlalchemy\n",
        "!pip install llama-cpp-python --upgrade  # Local LLM runtime\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O phi2.gguf https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q4_K_M.gguf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pdOB_o6XMDF",
        "outputId": "6f59b258-f62b-49c9-836d-27d789697983"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-22 09:06:59--  https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q4_K_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.167.112.25, 3.167.112.96, 3.167.112.45, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.167.112.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/6580aa20419afba19a692cc8/cb5d304e5b36d2f91430fff1530842167680b0958c4083b09e04d4dbf8cf7a08?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250722%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250722T090126Z&X-Amz-Expires=3600&X-Amz-Signature=809f1279f23ee713e697f123f0e6e18b9e3317c03142404547555a1be5177465&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27phi-2.Q4_K_M.gguf%3B+filename%3D%22phi-2.Q4_K_M.gguf%22%3B&x-id=GetObject&Expires=1753178486&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MzE3ODQ4Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NTgwYWEyMDQxOWFmYmExOWE2OTJjYzgvY2I1ZDMwNGU1YjM2ZDJmOTE0MzBmZmYxNTMwODQyMTY3NjgwYjA5NThjNDA4M2IwOWUwNGQ0ZGJmOGNmN2EwOCoifV19&Signature=jnyxBP1WwNvr%7EBeMcZHSnZrEigZLoX2CobbX56ggmzJuw%7ET47hL7g9WEgfYj894RTqXDOw5C31lCYdVVD1LWRPEi5Yi30h5L6Eaq6%7Ehb2yApnHIbe8wKD9Ff9%7EqWMyS98ywu06ObZEoT11wwf3WEJo9PEL2bf2WeMkfsU1y6Z6b79g5IIFzrRFuE3CikvyZuCS4CucVt2jG24dl7dbDE-7flRajEToCvuxUiw6NQm1Hcl%7EQh-sZJ-cmRlNHF4J3izSK2qWCkSs4mcwumlklihn4N6DE4TtE6IC9isItSHTxqlQ66OO4X1El9gmBzYwGsf4SJPS-hfZtXEzc5Z3jtVw__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-07-22 09:06:59--  https://cas-bridge.xethub.hf.co/xet-bridge-us/6580aa20419afba19a692cc8/cb5d304e5b36d2f91430fff1530842167680b0958c4083b09e04d4dbf8cf7a08?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250722%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250722T090126Z&X-Amz-Expires=3600&X-Amz-Signature=809f1279f23ee713e697f123f0e6e18b9e3317c03142404547555a1be5177465&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27phi-2.Q4_K_M.gguf%3B+filename%3D%22phi-2.Q4_K_M.gguf%22%3B&x-id=GetObject&Expires=1753178486&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MzE3ODQ4Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NTgwYWEyMDQxOWFmYmExOWE2OTJjYzgvY2I1ZDMwNGU1YjM2ZDJmOTE0MzBmZmYxNTMwODQyMTY3NjgwYjA5NThjNDA4M2IwOWUwNGQ0ZGJmOGNmN2EwOCoifV19&Signature=jnyxBP1WwNvr%7EBeMcZHSnZrEigZLoX2CobbX56ggmzJuw%7ET47hL7g9WEgfYj894RTqXDOw5C31lCYdVVD1LWRPEi5Yi30h5L6Eaq6%7Ehb2yApnHIbe8wKD9Ff9%7EqWMyS98ywu06ObZEoT11wwf3WEJo9PEL2bf2WeMkfsU1y6Z6b79g5IIFzrRFuE3CikvyZuCS4CucVt2jG24dl7dbDE-7flRajEToCvuxUiw6NQm1Hcl%7EQh-sZJ-cmRlNHF4J3izSK2qWCkSs4mcwumlklihn4N6DE4TtE6IC9isItSHTxqlQ66OO4X1El9gmBzYwGsf4SJPS-hfZtXEzc5Z3jtVw__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.165.98.81, 18.165.98.65, 18.165.98.73, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.165.98.81|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1789239136 (1.7G)\n",
            "Saving to: ‘phi2.gguf’\n",
            "\n",
            "phi2.gguf           100%[===================>]   1.67G   243MB/s    in 9.9s    \n",
            "\n",
            "2025-07-22 09:07:09 (172 MB/s) - ‘phi2.gguf’ saved [1789239136/1789239136]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama(\n",
        "    model_path=\"phi2.gguf\",\n",
        "    n_ctx=2048,\n",
        "    n_threads=4,\n",
        ")\n",
        "\n",
        "\n",
        "response = llm(\"Q: What is SQL?\\nA:\", max_tokens=100, stop=[\"\\n\"])\n",
        "print(response[\"choices\"][0][\"text\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJwwburwYsNQ",
        "outputId": "ad949d01-cdf7-4ca5-e7c5-5f1ddc734d64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 325 tensors from phi2.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = phi2\n",
            "llama_model_loader: - kv   1:                               general.name str              = Phi2\n",
            "llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048\n",
            "llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2560\n",
            "llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 10240\n",
            "llama_model_loader: - kv   5:                           phi2.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = [\"Ġ t\", \"Ġ a\", \"h e\", \"i n\", \"r e\",...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  195 tensors\n",
            "llama_model_loader: - type q4_K:   81 tensors\n",
            "llama_model_loader: - type q5_K:   32 tensors\n",
            "llama_model_loader: - type q6_K:   17 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 1.66 GiB (5.14 BPW) \n",
            "load: missing pre-tokenizer type, using: 'default'\n",
            "load:                                             \n",
            "load: ************************************        \n",
            "load: GENERATION QUALITY WILL BE DEGRADED!        \n",
            "load: CONSIDER REGENERATING THE MODEL             \n",
            "load: ************************************        \n",
            "load:                                             \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: special tokens cache size = 944\n",
            "load: token to piece cache size = 0.3151 MB\n",
            "print_info: arch             = phi2\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 2048\n",
            "print_info: n_embd           = 2560\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 32\n",
            "print_info: n_rot            = 32\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 80\n",
            "print_info: n_embd_head_v    = 80\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 2560\n",
            "print_info: n_embd_v_gqa     = 2560\n",
            "print_info: f_norm_eps       = 1.0e-05\n",
            "print_info: f_norm_rms_eps   = 0.0e+00\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 10240\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 2048\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 3B\n",
            "print_info: model params     = 2.78 B\n",
            "print_info: general.name     = Phi2\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 51200\n",
            "print_info: n_merges         = 50000\n",
            "print_info: BOS token        = 50256 '<|endoftext|>'\n",
            "print_info: EOS token        = 50256 '<|endoftext|>'\n",
            "print_info: EOT token        = 50256 '<|endoftext|>'\n",
            "print_info: UNK token        = 50256 '<|endoftext|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: EOG token        = 50256 '<|endoftext|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q4_K) (and 244 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
            "load_tensors:   CPU_REPACK model buffer size =   787.50 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =  1704.63 MiB\n",
            "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.6.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.12.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.21.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.24.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.27.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.29.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n",
            "..............................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 2048\n",
            "llama_context: n_ctx_per_seq = 2048\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: freq_base     = 10000.0\n",
            "llama_context: freq_scale    = 1\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     0.20 MiB\n",
            "create_memory: n_ctx = 2048 (padded)\n",
            "llama_kv_cache_unified: layer   0: dev = CPU\n",
            "llama_kv_cache_unified: layer   1: dev = CPU\n",
            "llama_kv_cache_unified: layer   2: dev = CPU\n",
            "llama_kv_cache_unified: layer   3: dev = CPU\n",
            "llama_kv_cache_unified: layer   4: dev = CPU\n",
            "llama_kv_cache_unified: layer   5: dev = CPU\n",
            "llama_kv_cache_unified: layer   6: dev = CPU\n",
            "llama_kv_cache_unified: layer   7: dev = CPU\n",
            "llama_kv_cache_unified: layer   8: dev = CPU\n",
            "llama_kv_cache_unified: layer   9: dev = CPU\n",
            "llama_kv_cache_unified: layer  10: dev = CPU\n",
            "llama_kv_cache_unified: layer  11: dev = CPU\n",
            "llama_kv_cache_unified: layer  12: dev = CPU\n",
            "llama_kv_cache_unified: layer  13: dev = CPU\n",
            "llama_kv_cache_unified: layer  14: dev = CPU\n",
            "llama_kv_cache_unified: layer  15: dev = CPU\n",
            "llama_kv_cache_unified: layer  16: dev = CPU\n",
            "llama_kv_cache_unified: layer  17: dev = CPU\n",
            "llama_kv_cache_unified: layer  18: dev = CPU\n",
            "llama_kv_cache_unified: layer  19: dev = CPU\n",
            "llama_kv_cache_unified: layer  20: dev = CPU\n",
            "llama_kv_cache_unified: layer  21: dev = CPU\n",
            "llama_kv_cache_unified: layer  22: dev = CPU\n",
            "llama_kv_cache_unified: layer  23: dev = CPU\n",
            "llama_kv_cache_unified: layer  24: dev = CPU\n",
            "llama_kv_cache_unified: layer  25: dev = CPU\n",
            "llama_kv_cache_unified: layer  26: dev = CPU\n",
            "llama_kv_cache_unified: layer  27: dev = CPU\n",
            "llama_kv_cache_unified: layer  28: dev = CPU\n",
            "llama_kv_cache_unified: layer  29: dev = CPU\n",
            "llama_kv_cache_unified: layer  30: dev = CPU\n",
            "llama_kv_cache_unified: layer  31: dev = CPU\n",
            "llama_kv_cache_unified:        CPU KV buffer size =   640.00 MiB\n",
            "llama_kv_cache_unified: size =  640.00 MiB (  2048 cells,  32 layers,  1 seqs), K (f16):  320.00 MiB, V (f16):  320.00 MiB\n",
            "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 1\n",
            "llama_context: max_nodes = 65536\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:        CPU compute buffer size =   162.01 MiB\n",
            "llama_context: graph nodes  = 1257\n",
            "llama_context: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '50256', 'tokenizer.ggml.eos_token_id': '50256', 'tokenizer.ggml.bos_token_id': '50256', 'general.architecture': 'phi2', 'general.name': 'Phi2', 'phi2.context_length': '2048', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.add_bos_token': 'false', 'phi2.embedding_length': '2560', 'phi2.attention.head_count': '32', 'phi2.attention.head_count_kv': '32', 'phi2.feed_forward_length': '10240', 'phi2.attention.layer_norm_epsilon': '0.000010', 'phi2.block_count': '32', 'phi2.rope.dimension_count': '32', 'general.file_type': '15'}\n",
            "Using fallback chat format: llama-2\n",
            "llama_perf_context_print:        load time =    1391.01 ms\n",
            "llama_perf_context_print: prompt eval time =    1390.80 ms /     9 tokens (  154.53 ms per token,     6.47 tokens per second)\n",
            "llama_perf_context_print:        eval time =   11670.08 ms /    43 runs   (  271.40 ms per token,     3.68 tokens per second)\n",
            "llama_perf_context_print:       total time =   13093.98 ms /    52 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " SQL, or Structured Query Language, is a programming language used for managing and manipulating data in relational databases. It allows users to communicate with databases, retrieve and modify data, and perform various operations on the data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "4xVcDsFhYxR6",
        "outputId": "ec5ef8e0-45c9-4139-8783-531d65a0842a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b7f8a312-3fb1-4e39-b861-3687c93eaf4c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b7f8a312-3fb1-4e39-b861-3687c93eaf4c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Product-Level Total Sales and Metrics (mapped).xlsx to Product-Level Total Sales and Metrics (mapped).xlsx\n",
            "Saving Product-Level Ad Sales and Metrics (mapped).xlsx to Product-Level Ad Sales and Metrics (mapped).xlsx\n",
            "Saving Product-Level Eligibility Table (mapped).xlsx to Product-Level Eligibility Table (mapped).xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect(\"ecommerce.db\")\n",
        "\n",
        "sales_df = pd.read_excel(\"Product-Level Total Sales and Metrics (mapped).xlsx\")\n",
        "ads_df = pd.read_excel(\"Product-Level Ad Sales and Metrics (mapped).xlsx\")\n",
        "eligibility_df = pd.read_excel(\"Product-Level Eligibility Table (mapped).xlsx\")\n",
        "\n",
        "# Write to SQL\n",
        "sales_df.to_sql(\"total_sales\", conn, if_exists=\"replace\", index=False)\n",
        "ads_df.to_sql(\"ad_sales\", conn, if_exists=\"replace\", index=False)\n",
        "eligibility_df.to_sql(\"eligibility\", conn, if_exists=\"replace\", index=False)\n",
        "\n",
        "print(\"Tables created successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oguKmj6gZCaO",
        "outputId": "5ec2d985-cb42-4e3c-ecfa-d6c2efc3b7f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tables created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "from llama_cpp import Llama\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "conn = sqlite3.connect(\"ecommerce.db\", check_same_thread=False)\n",
        "\n",
        "llm = Llama(model_path=\"phi2.gguf\", n_ctx=2048, n_threads=4)\n",
        "\n",
        "class Question(BaseModel):\n",
        "    query: str\n",
        "\n",
        "@app.post(\"/ask\")\n",
        "async def ask_question(q: Question):\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an expert data analyst working with an SQLite database containing the following tables: total_sales, ad_sales, eligibility.\n",
        "\n",
        "Convert the following user question into a correct SQL query using these tables.\n",
        "\n",
        "User question: {q.query}\n",
        "\n",
        "SQL query:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "    response = llm(prompt, stop=[\"\\n\"], max_tokens=200)\n",
        "    sql_query = response[\"choices\"][0][\"text\"].strip()\n",
        "\n",
        "    try:\n",
        "        result_df = pd.read_sql(sql_query, conn)\n",
        "        result_text = result_df.to_markdown()\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e), \"sql\": sql_query}\n",
        "\n",
        "    return {\n",
        "        \"sql_query\": sql_query,\n",
        "        \"result\": result_text\n",
        "    }\n",
        "\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=7860)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_0qHQmWZd05",
        "outputId": "1b41d8a3-4ef1-4fc3-9e8c-299080cc7698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 325 tensors from phi2.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = phi2\n",
            "llama_model_loader: - kv   1:                               general.name str              = Phi2\n",
            "llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048\n",
            "llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2560\n",
            "llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 10240\n",
            "llama_model_loader: - kv   5:                           phi2.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = [\"Ġ t\", \"Ġ a\", \"h e\", \"i n\", \"r e\",...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  195 tensors\n",
            "llama_model_loader: - type q4_K:   81 tensors\n",
            "llama_model_loader: - type q5_K:   32 tensors\n",
            "llama_model_loader: - type q6_K:   17 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 1.66 GiB (5.14 BPW) \n",
            "load: missing pre-tokenizer type, using: 'default'\n",
            "load:                                             \n",
            "load: ************************************        \n",
            "load: GENERATION QUALITY WILL BE DEGRADED!        \n",
            "load: CONSIDER REGENERATING THE MODEL             \n",
            "load: ************************************        \n",
            "load:                                             \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: special tokens cache size = 944\n",
            "load: token to piece cache size = 0.3151 MB\n",
            "print_info: arch             = phi2\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 2048\n",
            "print_info: n_embd           = 2560\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 32\n",
            "print_info: n_rot            = 32\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 80\n",
            "print_info: n_embd_head_v    = 80\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 2560\n",
            "print_info: n_embd_v_gqa     = 2560\n",
            "print_info: f_norm_eps       = 1.0e-05\n",
            "print_info: f_norm_rms_eps   = 0.0e+00\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 10240\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 2048\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 3B\n",
            "print_info: model params     = 2.78 B\n",
            "print_info: general.name     = Phi2\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 51200\n",
            "print_info: n_merges         = 50000\n",
            "print_info: BOS token        = 50256 '<|endoftext|>'\n",
            "print_info: EOS token        = 50256 '<|endoftext|>'\n",
            "print_info: EOT token        = 50256 '<|endoftext|>'\n",
            "print_info: UNK token        = 50256 '<|endoftext|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: EOG token        = 50256 '<|endoftext|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q4_K) (and 244 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
            "load_tensors:   CPU_REPACK model buffer size =   787.50 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =  1704.63 MiB\n",
            "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.6.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.12.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.21.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.24.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.27.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.29.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n",
            "..............................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 2048\n",
            "llama_context: n_ctx_per_seq = 2048\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: freq_base     = 10000.0\n",
            "llama_context: freq_scale    = 1\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     0.20 MiB\n",
            "create_memory: n_ctx = 2048 (padded)\n",
            "llama_kv_cache_unified: layer   0: dev = CPU\n",
            "llama_kv_cache_unified: layer   1: dev = CPU\n",
            "llama_kv_cache_unified: layer   2: dev = CPU\n",
            "llama_kv_cache_unified: layer   3: dev = CPU\n",
            "llama_kv_cache_unified: layer   4: dev = CPU\n",
            "llama_kv_cache_unified: layer   5: dev = CPU\n",
            "llama_kv_cache_unified: layer   6: dev = CPU\n",
            "llama_kv_cache_unified: layer   7: dev = CPU\n",
            "llama_kv_cache_unified: layer   8: dev = CPU\n",
            "llama_kv_cache_unified: layer   9: dev = CPU\n",
            "llama_kv_cache_unified: layer  10: dev = CPU\n",
            "llama_kv_cache_unified: layer  11: dev = CPU\n",
            "llama_kv_cache_unified: layer  12: dev = CPU\n",
            "llama_kv_cache_unified: layer  13: dev = CPU\n",
            "llama_kv_cache_unified: layer  14: dev = CPU\n",
            "llama_kv_cache_unified: layer  15: dev = CPU\n",
            "llama_kv_cache_unified: layer  16: dev = CPU\n",
            "llama_kv_cache_unified: layer  17: dev = CPU\n",
            "llama_kv_cache_unified: layer  18: dev = CPU\n",
            "llama_kv_cache_unified: layer  19: dev = CPU\n",
            "llama_kv_cache_unified: layer  20: dev = CPU\n",
            "llama_kv_cache_unified: layer  21: dev = CPU\n",
            "llama_kv_cache_unified: layer  22: dev = CPU\n",
            "llama_kv_cache_unified: layer  23: dev = CPU\n",
            "llama_kv_cache_unified: layer  24: dev = CPU\n",
            "llama_kv_cache_unified: layer  25: dev = CPU\n",
            "llama_kv_cache_unified: layer  26: dev = CPU\n",
            "llama_kv_cache_unified: layer  27: dev = CPU\n",
            "llama_kv_cache_unified: layer  28: dev = CPU\n",
            "llama_kv_cache_unified: layer  29: dev = CPU\n",
            "llama_kv_cache_unified: layer  30: dev = CPU\n",
            "llama_kv_cache_unified: layer  31: dev = CPU\n",
            "llama_kv_cache_unified:        CPU KV buffer size =   640.00 MiB\n",
            "llama_kv_cache_unified: size =  640.00 MiB (  2048 cells,  32 layers,  1 seqs), K (f16):  320.00 MiB, V (f16):  320.00 MiB\n",
            "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 1\n",
            "llama_context: max_nodes = 65536\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:        CPU compute buffer size =   162.01 MiB\n",
            "llama_context: graph nodes  = 1257\n",
            "llama_context: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '50256', 'tokenizer.ggml.eos_token_id': '50256', 'tokenizer.ggml.bos_token_id': '50256', 'general.architecture': 'phi2', 'general.name': 'Phi2', 'phi2.context_length': '2048', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.add_bos_token': 'false', 'phi2.embedding_length': '2560', 'phi2.attention.head_count': '32', 'phi2.attention.head_count_kv': '32', 'phi2.feed_forward_length': '10240', 'phi2.attention.layer_norm_epsilon': '0.000010', 'phi2.block_count': '32', 'phi2.rope.dimension_count': '32', 'general.file_type': '15'}\n",
            "Using fallback chat format: llama-2\n",
            "INFO:     Started server process [4865]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:7860 (Press CTRL+C to quit)\n",
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [4865]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gTArGp1ZtcL",
        "outputId": "18bf493e-a882-40b1-8e96-59e81e4126b8"
      },
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "import re\n",
        "from llama_cpp import Llama\n",
        "\n",
        "\n",
        "llm = Llama(model_path=\"phi2.gguf\", n_ctx=2048)\n",
        "\n",
        "\n",
        "conn = sqlite3.connect(\"ecommerce.db\")\n",
        "\n",
        "def clean_sql_output(text):\n",
        "\n",
        "    text = re.sub(r\"```(sql)?\", \"\", text)\n",
        "    text = text.strip().split(\"\\n\\n\")[0]\n",
        "    match = re.search(r\"SELECT.*\", text, re.IGNORECASE | re.DOTALL)\n",
        "    if match:\n",
        "        return match.group(0).strip()\n",
        "    return text.strip()\n",
        "\n",
        "def build_prompt(user_question):\n",
        "    return f\"\"\"\n",
        "Generate a valid SQLite SQL query for the following question, using the tables:\n",
        "total_sales(date, item_id, total_sales, total_units_ordered)\n",
        "ad_sales(date, item_id, ad_sales, impressions, ad_spend, clicks, units_sold)\n",
        "eligibility(eligibility_datetime_utc, item_id, eligibility, message)\n",
        "\n",
        "Return *only* the SQL query and nothing else. Do not include any explanations, markdown, or other text.\n",
        "\n",
        "Examples:\n",
        "Q: What is my total sales?\n",
        "A: SELECT SUM(total_sales) FROM total_sales;\n",
        "\n",
        "Q: Calculate the RoAS (Return on Ad Spend).\n",
        "A: SELECT CAST(SUM(ad_sales) AS REAL) / NULLIF(SUM(ad_spend), 0) AS roas FROM ad_sales;\n",
        "\n",
        "Q: Which product had the highest CPC (Cost Per Click)?\n",
        "A: SELECT item_id, CAST(ad_spend AS REAL) / NULLIF(clicks, 0) AS cpc FROM ad_sales ORDER BY cpc DESC LIMIT 1;\n",
        "\n",
        "Q: How many distinct items are there?\n",
        "A: SELECT COUNT(DISTINCT item_id) FROM total_sales;\n",
        "\n",
        "Q: What is the average ad spend?\n",
        "A: SELECT AVG(ad_spend) FROM ad_sales;\n",
        "\n",
        "Q: Show the total units ordered per item.\n",
        "A: SELECT item_id, SUM(total_units_ordered) FROM total_sales GROUP BY item_id;\n",
        "\n",
        "Q: What is the latest eligibility status for each item?\n",
        "A: SELECT item_id, eligibility, message FROM eligibility WHERE eligibility_datetime_utc IN (SELECT MAX(eligibility_datetime_utc) FROM eligibility GROUP BY item_id);\n",
        "\n",
        "\n",
        "Now answer this:\n",
        "Q: {user_question}\n",
        "A:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def answer_question(user_question):\n",
        "    prompt = build_prompt(user_question)\n",
        "    output = llm(prompt, max_tokens=256)\n",
        "    sql_text = output[\"choices\"][0][\"text\"]\n",
        "    sql_query = clean_sql_output(sql_text)\n",
        "\n",
        "    print(\"\\n🧠 LLM Output:\\n\", sql_query)\n",
        "\n",
        "    try:\n",
        "        result_df = pd.read_sql_query(sql_query, conn)\n",
        "        print(\"\\n📊 Query Result:\\n\", result_df)\n",
        "    except Exception as e:\n",
        "        print(\"❌ SQL Execution Error:\", e)\n",
        "\n",
        "for q in [\n",
        "    \"What is my total sales?\",\n",
        "    \"Calculate the RoAS (Return on Ad Spend).\",\n",
        "    \"Which product had the highest CPC (Cost Per Click)?\"\n",
        "]:\n",
        "    print(\"\\n\\n==============================\")\n",
        "    print(f\"🔍 Question: {q}\")\n",
        "    answer_question(q)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 325 tensors from phi2.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = phi2\n",
            "llama_model_loader: - kv   1:                               general.name str              = Phi2\n",
            "llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048\n",
            "llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2560\n",
            "llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 10240\n",
            "llama_model_loader: - kv   5:                           phi2.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = [\"Ġ t\", \"Ġ a\", \"h e\", \"i n\", \"r e\",...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  195 tensors\n",
            "llama_model_loader: - type q4_K:   81 tensors\n",
            "llama_model_loader: - type q5_K:   32 tensors\n",
            "llama_model_loader: - type q6_K:   17 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 1.66 GiB (5.14 BPW) \n",
            "load: missing pre-tokenizer type, using: 'default'\n",
            "load:                                             \n",
            "load: ************************************        \n",
            "load: GENERATION QUALITY WILL BE DEGRADED!        \n",
            "load: CONSIDER REGENERATING THE MODEL             \n",
            "load: ************************************        \n",
            "load:                                             \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: special tokens cache size = 944\n",
            "load: token to piece cache size = 0.3151 MB\n",
            "print_info: arch             = phi2\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 2048\n",
            "print_info: n_embd           = 2560\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 32\n",
            "print_info: n_rot            = 32\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 80\n",
            "print_info: n_embd_head_v    = 80\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 2560\n",
            "print_info: n_embd_v_gqa     = 2560\n",
            "print_info: f_norm_eps       = 1.0e-05\n",
            "print_info: f_norm_rms_eps   = 0.0e+00\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 10240\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 2048\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 3B\n",
            "print_info: model params     = 2.78 B\n",
            "print_info: general.name     = Phi2\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 51200\n",
            "print_info: n_merges         = 50000\n",
            "print_info: BOS token        = 50256 '<|endoftext|>'\n",
            "print_info: EOS token        = 50256 '<|endoftext|>'\n",
            "print_info: EOT token        = 50256 '<|endoftext|>'\n",
            "print_info: UNK token        = 50256 '<|endoftext|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: EOG token        = 50256 '<|endoftext|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q4_K) (and 244 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
            "load_tensors:   CPU_REPACK model buffer size =   787.50 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =  1704.63 MiB\n",
            "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.6.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.12.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.21.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.24.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.27.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.29.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n",
            "..............................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 2048\n",
            "llama_context: n_ctx_per_seq = 2048\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: freq_base     = 10000.0\n",
            "llama_context: freq_scale    = 1\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     0.20 MiB\n",
            "create_memory: n_ctx = 2048 (padded)\n",
            "llama_kv_cache_unified: layer   0: dev = CPU\n",
            "llama_kv_cache_unified: layer   1: dev = CPU\n",
            "llama_kv_cache_unified: layer   2: dev = CPU\n",
            "llama_kv_cache_unified: layer   3: dev = CPU\n",
            "llama_kv_cache_unified: layer   4: dev = CPU\n",
            "llama_kv_cache_unified: layer   5: dev = CPU\n",
            "llama_kv_cache_unified: layer   6: dev = CPU\n",
            "llama_kv_cache_unified: layer   7: dev = CPU\n",
            "llama_kv_cache_unified: layer   8: dev = CPU\n",
            "llama_kv_cache_unified: layer   9: dev = CPU\n",
            "llama_kv_cache_unified: layer  10: dev = CPU\n",
            "llama_kv_cache_unified: layer  11: dev = CPU\n",
            "llama_kv_cache_unified: layer  12: dev = CPU\n",
            "llama_kv_cache_unified: layer  13: dev = CPU\n",
            "llama_kv_cache_unified: layer  14: dev = CPU\n",
            "llama_kv_cache_unified: layer  15: dev = CPU\n",
            "llama_kv_cache_unified: layer  16: dev = CPU\n",
            "llama_kv_cache_unified: layer  17: dev = CPU\n",
            "llama_kv_cache_unified: layer  18: dev = CPU\n",
            "llama_kv_cache_unified: layer  19: dev = CPU\n",
            "llama_kv_cache_unified: layer  20: dev = CPU\n",
            "llama_kv_cache_unified: layer  21: dev = CPU\n",
            "llama_kv_cache_unified: layer  22: dev = CPU\n",
            "llama_kv_cache_unified: layer  23: dev = CPU\n",
            "llama_kv_cache_unified: layer  24: dev = CPU\n",
            "llama_kv_cache_unified: layer  25: dev = CPU\n",
            "llama_kv_cache_unified: layer  26: dev = CPU\n",
            "llama_kv_cache_unified: layer  27: dev = CPU\n",
            "llama_kv_cache_unified: layer  28: dev = CPU\n",
            "llama_kv_cache_unified: layer  29: dev = CPU\n",
            "llama_kv_cache_unified: layer  30: dev = CPU\n",
            "llama_kv_cache_unified: layer  31: dev = CPU\n",
            "llama_kv_cache_unified:        CPU KV buffer size =   640.00 MiB\n",
            "llama_kv_cache_unified: size =  640.00 MiB (  2048 cells,  32 layers,  1 seqs), K (f16):  320.00 MiB, V (f16):  320.00 MiB\n",
            "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 1\n",
            "llama_context: max_nodes = 65536\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:        CPU compute buffer size =   162.01 MiB\n",
            "llama_context: graph nodes  = 1257\n",
            "llama_context: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '50256', 'tokenizer.ggml.eos_token_id': '50256', 'tokenizer.ggml.bos_token_id': '50256', 'general.architecture': 'phi2', 'general.name': 'Phi2', 'phi2.context_length': '2048', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.add_bos_token': 'false', 'phi2.embedding_length': '2560', 'phi2.attention.head_count': '32', 'phi2.attention.head_count_kv': '32', 'phi2.feed_forward_length': '10240', 'phi2.attention.layer_norm_epsilon': '0.000010', 'phi2.block_count': '32', 'phi2.rope.dimension_count': '32', 'general.file_type': '15'}\n",
            "Using fallback chat format: llama-2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "==============================\n",
            "🔍 Question: What is my total sales?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   67765.27 ms\n",
            "llama_perf_context_print: prompt eval time =   67764.17 ms /   451 tokens (  150.25 ms per token,     6.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =   96983.51 ms /   255 runs   (  380.33 ms per token,     2.63 tokens per second)\n",
            "llama_perf_context_print:       total time =  164999.73 ms /   706 tokens\n",
            "Llama.generate: 441 prefix-match hit, remaining 16 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧠 LLM Output:\n",
            " SELECT SUM(total_sales) FROM total_sales;\n",
            "\n",
            "📊 Query Result:\n",
            "    SUM(total_sales)\n",
            "0        1004904.56\n",
            "\n",
            "\n",
            "==============================\n",
            "🔍 Question: Calculate the RoAS (Return on Ad Spend).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   67765.27 ms\n",
            "llama_perf_context_print: prompt eval time =    2419.76 ms /    16 tokens (  151.24 ms per token,     6.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =   45229.46 ms /   135 runs   (  335.03 ms per token,     2.98 tokens per second)\n",
            "llama_perf_context_print:       total time =   47757.86 ms /   151 tokens\n",
            "Llama.generate: 441 prefix-match hit, remaining 16 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧠 LLM Output:\n",
            " SELECT CAST(SUM(ad_sales) AS REAL)  / NULLIF(SUM(ad_spend), \n",
            "0) AS roas FROM ad_sales;\n",
            "\n",
            "📊 Query Result:\n",
            "        roas\n",
            "0  7.915767\n",
            "\n",
            "\n",
            "==============================\n",
            "🔍 Question: Which product had the highest CPC (Cost Per Click)?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   67765.27 ms\n",
            "llama_perf_context_print: prompt eval time =    2127.21 ms /    16 tokens (  132.95 ms per token,     7.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =   17998.89 ms /    46 runs   (  391.28 ms per token,     2.56 tokens per second)\n",
            "llama_perf_context_print:       total time =   20161.60 ms /    62 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧠 LLM Output:\n",
            " Here is a valid SQLite SQL query that can be used to answer the question.\n",
            "❌ SQL Execution Error: Execution failed on sql 'Here is a valid SQLite SQL query that can be used to answer the question.': near \"Here\": syntax error\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_question(\"What is my total sales?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neezcJVaoYPo",
        "outputId": "2c1dcaad-059d-4f0a-cf94-52a7e4252395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 441 prefix-match hit, remaining 10 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   64906.06 ms\n",
            "llama_perf_context_print: prompt eval time =    1653.00 ms /    10 tokens (  165.30 ms per token,     6.05 tokens per second)\n",
            "llama_perf_context_print:        eval time =   25376.39 ms /    64 runs   (  396.51 ms per token,     2.52 tokens per second)\n",
            "llama_perf_context_print:       total time =   27084.41 ms /    74 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧠 LLM Output:\n",
            " SELECT SUM(total_sales) FROM total_sales;\n",
            "\n",
            "📊 Query Result:\n",
            "    SUM(total_sales)\n",
            "0        1004904.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_question(\"Calculate the RoAS (Return on Ad Spend).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhWyjsMIrsic",
        "outputId": "36a1e983-9fca-4600-c575-e7dca6c716ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 441 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   64906.06 ms\n",
            "llama_perf_context_print: prompt eval time =    4009.81 ms /    16 tokens (  250.61 ms per token,     3.99 tokens per second)\n",
            "llama_perf_context_print:        eval time =   87575.54 ms /   216 runs   (  405.44 ms per token,     2.47 tokens per second)\n",
            "llama_perf_context_print:       total time =   91788.99 ms /   232 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧠 LLM Output:\n",
            " SELECT CAST(SUM(ad_sales) AS REAL) \n",
            "    / NULLIF(SUM(ad_spend), \n",
            "    0) AS roas \n",
            "FROM ad_sales;\n",
            "\n",
            "📊 Query Result:\n",
            "        roas\n",
            "0  7.915767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_question(\"Which product had the highest CPC (Cost Per Click)?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ7_x2DzvPoG",
        "outputId": "00e11691-8c4d-4bb5-80ca-fbf719cefeac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 441 prefix-match hit, remaining 16 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   64906.06 ms\n",
            "llama_perf_context_print: prompt eval time =    2596.76 ms /    16 tokens (  162.30 ms per token,     6.16 tokens per second)\n",
            "llama_perf_context_print:        eval time =   56320.15 ms /   141 runs   (  399.43 ms per token,     2.50 tokens per second)\n",
            "llama_perf_context_print:       total time =   59042.44 ms /   157 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧠 LLM Output:\n",
            " SELECT item_id, CAST(ad_spend AS REAL) \n",
            "  / NULLIF(clicks,\n",
            "❌ SQL Execution Error: Execution failed on sql 'SELECT item_id, CAST(ad_spend AS REAL) \n",
            "  / NULLIF(clicks,': incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OOfCXtSlocnF"
      }
    }
  ]
}